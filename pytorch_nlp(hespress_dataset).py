# -*- coding: utf-8 -*-
"""PyTorch_Nlp(Hespress dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GroAEKLt9rySe5rxWqIaz3XetUyZsxXk

## Install packages
"""

# [1] Set Kaggle Credentials (Replace with your actual username and key)
import os
os.environ['KAGGLE_USERNAME'] = "ashrakattamer"
os.environ['KAGGLE_KEY'] = "2b208f90a5ec718e07dd9a8c01511344" # Your API key

# [3] Download the Hespress Dataset
!kaggle datasets download -d tariqmassaoudi/hespress

# Commented out IPython magic to ensure Python compatibility.
# # [2] Install Dependencies and Unzip
# %%capture
# # This command is often used to suppress the output of the pip install commands
# !pip install pyarabic
# !pip install pystemmer
# 
# # Unzip the downloaded dataset into a new directory
# !unzip hespress.zip -d hespressData

"""## Import Packages"""

import os
import torch
import pyarabic.araby as ar
import re
import Stemmer
import functools
import operator
import string
import gc
import random
import seaborn as sns
import matplotlib.pyplot as plt
import glob
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score, recall_score
from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler
from torch.optim import Adam
from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score
from sklearn.utils import resample
import torch.nn as nn

import logging
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# if there is a GPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"There are {torch.cuda.device_count()} GPU(s) available.")
    print("Device name:", torch.cuda.get_device_name(0))
else:
  print("No GPU available, using the CPU instead.")
  device = torch.device("cpu")

from google.colab import drive
drive.mount('/content/drive')

# check all files in dataset
for dirname,_, filename in os.walk('hespressData'):
    for file in filename:
        print(os.path.join(dirname, file))

csv_stories=[csv_file for csv_file in glob.glob('hespressData/*.csv') if 'stories' in csv_file]

import pandas as pd

stories_df=pd.DataFrame()
for csv_file in csv_stories:
    df=pd.read_csv(csv_file,index_col=0,encoding='utf-8')
    stories_df=pd.concat([stories_df,df],ignore_index=True)

stories_df.head()

stories_df.value_counts('topic')

"""## Preparing and cleaning data"""

stories_df.isnull().sum()

stories_df.drop(columns=stories_df.columns[0:4],axis=1,inplace=True)

topics = {
    "art-et-culture": "Art and Culture",
    "economie": "Economy",
    "faits-divers": "Miscellaneous/Current Events",
    "marocains-du-monde": "Moroccans of the World",
    "medias": "Media",
    "orbites": "Orbits",
    "politique": "Politics",
    "regions": "Regions",
    "societe": "Society",
    "sport": "Sport",
    "tamazight": "Tamazight"
}

stories_df['topic'] = stories_df['topic'].replace(topics)

stories_df.head()

st=Stemmer.Stemmer('arabic')

import re # Note: 're' is assumed to be imported, as per usage
# NOTE: This function relies on a stemmer object named 'st' being defined elsewhere.

# def clean(text):
#     # Remove URLs (http/https and various forms)
#     text = re.sub(r'^https?://\S+/\S+[/r/n]', '', str(text), flags=re.MULTILINE)
#     text = re.sub(r'^http\S+/\S+[/r/n]', '', str(text), flags=re.MULTILINE)
#     text = re.sub(r'http\S+', "", str(text))
#     text = re.sub(r'https\S+', "", str(text))

#     # Remove various types of whitespaces, numbers, and punctuation
#     text = re.sub(r'(\s\s+)', " ", str(text))
#     text = re.sub(r'(\d+)', "", str(text))
#     text = re.sub(r'^\d+\s+\d+\b\d+\b', " ", str(text))
#     text = re.sub(r'\d+', "", str(text))

#     # Remove specific punctuation and symbols
#     text = text.replace("#", " ")
#     text = text.replace("@", " ")
#     text = text.replace("_", " ")

#     # Normalize repeated characters (e.g., "aaaa" -> "a")
#     text = re.sub(r'(.+)\1{1,}', r'\1', text)

#     # Stemming (Apply stemWord to every word and join with spaces)
#     # NOTE: This requires 'st' (the stemmer object) to be defined.
#     text_stem = " ".join([st.stemWord(i) for i in text.split()])

#     # Append the stemmed text to the original text (creating a token mix)
#     text = text + " " + text_stem

#     # Replace common Arabic/other punctuation and symbols with a space
#     text = text.replace("[", " ")
#     text = text.replace("]", " ")
#     text = text.replace("(", " ")
#     text = text.replace(")", " ")
#     text = text.replace("{", " ")
#     text = text.replace("}", " ")

#     return text

def clean(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', str(text))
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[_@#]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    # Limit to first 100 tokens to save memory
    text_stem = " ".join(st.stemWord(w) for w in words[:100])
    return text_stem

stories_df['story'] = stories_df['story'].apply(lambda x:clean(x))

topic_ids = {
    "Art and Culture": 0,
    "Economy": 1,
    "Miscellaneous/Current Events": 2,
    "Moroccans of the World": 3,
    "Media": 4,
    "Orbits": 5,
    "Politics": 6,
    "Regions": 7,
    "Society": 8,
    "Sport": 9,
    "Tamazight": 10
}

# Create the 'topic_id' column by mapping values from the 'topic' column
stories_df['topic_id'] = stories_df['topic'].replace(topic_ids).astype(int)

stories_df.info()

stories_df.head()

"""## EDA"""

topics=stories_df.topic.value_counts()
plt.figure(figsize=(10,5))
plt.title('Topics Distribution')
plt.pie(topics,labels=topics.index,autopct='%1.1f%%')

"""## Loading the Data"""

lenghts=stories_df['story'].apply(len)

max_lenght=lenghts.max()
min_lenght=lenghts.min()
mean_lenght=lenghts.mean()
median_lenght=lenghts.median()
std_lenght=lenghts.std()
print(f'Max Lenght: {max_lenght}\nMin Lenght: {min_lenght}\nMean Lenght: {mean_lenght}\nMedian Lenght: {median_lenght}\nStandard Deviation: {std_lenght}')

import matplotlib.pyplot as plt

plt.hist(lenghts,bins=20)
plt.xlabel('Lenght')
plt.ylabel('Frequency')
plt.title('Lenght Distribution')
plt.show()

max_len=lenghts.quantile(0.75)

print(f'Max Lenght: {int(max_len)}')

model_name="asafaya/bert-base-arabic"
max_lenght=128
batch_size=8

stories_df=stories_df.sample(frac=1).reset_index(drop=True)

stories_df.reset_index(drop=True,inplace=True)

stories_df

# Assuming 'stories_df' is the final DataFrame containing all processed data

# 1. Training Set (First 7000 rows)
train_texts = stories_df.iloc[:7000]['story'].values
train_labels = stories_df.iloc[:7000]['topic_id'].values

# 2. Validation Set (Rows 7000 up to 9000)
valid_texts = stories_df.iloc[7000:9000]['story'].values
valid_labels = stories_df.iloc[7000:9000]['topic_id'].values

# 3. Test Set (From row 9000 to the end)
test_texts = stories_df.iloc[9000:]['story'].values
test_labels = stories_df.iloc[9000:]['topic_id'].values

tokenizer=AutoTokenizer.from_pretrained(model_name,max_length=max_lenght, truncation=True)

# conver texts to numbers
train_encodings=tokenizer(list(train_texts),truncation=True,padding='max_length',max_length=max_lenght)
valid_encodings=tokenizer(list(valid_texts),truncation=True,padding='max_length',max_length=max_lenght)
test_encodings=tokenizer(list(test_texts),truncation=True,padding='max_length',max_length=max_lenght)

class StoriesDataset(Dataset):
    def __init__(self,encodings,labels):
        self.encodings=encodings
        self.labels=labels
    def __getitem__(self,idx):
        item={key:torch.tensor(val[idx]) for key,val in self.encodings.items()}
        # Ensure correct data types for tensors
        item['input_ids'] = torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long)
        item['attention_mask'] = torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long)
        item['token_type_ids'] = torch.tensor(self.encodings['token_type_ids'][idx], dtype=torch.long)
        item['labels']=torch.tensor(self.labels[idx], dtype=torch.long)
        return item # Added return statement
    def __len__(self):
        return len(self.labels)

train_dataset=StoriesDataset(train_encodings,train_labels)
valid_dataset=StoriesDataset(valid_encodings,valid_labels)
test_dataset=StoriesDataset(test_encodings,test_labels)

train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)
valid_loader=DataLoader(valid_dataset,batch_size=batch_size,shuffle=False)
test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)

for batch in train_loader:
    print(batch)
    break

"""## Building BERTMODEL For Topic Classification"""

model=BertForSequenceClassification.from_pretrained(model_name,num_labels=11)

model.train()
optim=torch.optim.Adam(model.parameters(),lr=5e-5)
NUM_EPOCHS=10

# def compute_accuracy(model,data_loader,device):
#     with torch.no_grad():
#         correct_pred,num_examples=0,0
#         for batch_idx,batch in enumerate(data_loader):
#           input_ids=batch['input_ids'].to(device)
#           attention_mask=batch['attention_mask'].to(device)
#           labels=batch['labels'].to(device)
#           # forward pass
#           outputs=model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)
#           logits=outputs['logits']
#           # get predicted labels
#           predicted_labels=torch.argmax(logits,1)
#           num_examples+=labels.size(0)
#           correct_pred+=(predicted_labels==labels).sum().item()
#           num_examples+=labels.size(0)

#         return correct_pred.float()/num_examples*100
#         return accuracy

def compute_accuracy(model, data_loader, device):
    correct = 0
    total = 0
    model.eval()

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)

            correct += (preds == labels).sum().item()
            total += labels.size(0)

            del input_ids, attention_mask, labels, outputs, logits
            torch.cuda.empty_cache()

    return 100 * correct / total

import torch
from tqdm import tqdm

train_losses = []
train_accuracies = []
valid_accuracies = []

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device) # Move the model to the device

scaler = torch.cuda.amp.GradScaler()  # Mixed precision training

for epoch in range(NUM_EPOCHS):
    model.train()
    epoch_loss = 0.0

    train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}", unit="batch")

    for batch in train_progress:
        # Explicitly cast tensors to long before moving to device
        input_ids = batch["input_ids"].to(device, dtype=torch.long)
        attention_mask = batch["attention_mask"].to(device, dtype=torch.long)
        labels = batch["labels"].to(device, dtype=torch.long)

        optim.zero_grad()

        # ✅ Use mixed precision for less memory
        with torch.cuda.amp.autocast():
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss

        scaler.scale(loss).backward()
        scaler.step(optim)
        scaler.update()

        epoch_loss += loss.item()
        train_progress.set_postfix(loss=loss.item())

        # ✅ Free memory of local tensors
        del input_ids, attention_mask, labels, outputs
        torch.cuda.empty_cache()

    avg_loss = epoch_loss / len(train_loader)
    train_losses.append(avg_loss)

    # ===================================
    # EVALUATION PHASE
    # ===================================
    model.eval()
    with torch.no_grad():
        train_accuracy = compute_accuracy(model, train_loader, device)
        valid_accuracy = compute_accuracy(model, valid_loader, device)

    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)

    print(f"\nEpoch {epoch+1}/{NUM_EPOCHS} | "
          f"Train Acc: {train_accuracy:.2f}% | "
          f"Valid Acc: {valid_accuracy:.2f}%")

    # ✅ Clear cache each epoch
    torch.cuda.empty_cache()

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# plotting loss and accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(1,NUM_EPOCHS+1),train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

#Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(range(1,NUM_EPOCHS+1), train_accuracies, label='Training Accuracy')
plt.plot(range(1,NUM_EPOCHS+1), valid_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy(%)')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

"""## Save and load the model"""

# save the model
torch.save(model.state_dict(), 'model.pth')

#load the model
loaded_model=BertForSequenceClassification.from_pretrained(model_name,num_labels=11)
loaded_model.load_state_dict(torch.load('model.pth'))
loaded_model.to(device)

"""## Testing Model"""

loaded_model.eval()
loaded_model.to(device) # Use the device variable
print(f"Test accuracy: {compute_accuracy(loaded_model,test_loader,device):.2f}%") # Use the device variable

"""# Task
Evaluate the loaded model on custom data.
"""

import pandas as pd

"""## Load custom data

### Subtask:
Load your custom data into a pandas DataFrame. Ensure it has a column for the text and a column for the topic labels, similar to the original data.

**Reasoning**:
Create a dummy DataFrame with 'story' and 'topic' columns since the custom data file was not found in previous attempts.
"""

# Create a larger dummy DataFrame with 'story' and 'topic' columns
data = {'story': [
    "تناول وزير الاقتصاد والمالية اليوم خطة الحكومة الجديدة لتعزيز النمو الاقتصادي في البلاد.",
    "فاز المنتخب الوطني لكرة القدم بمباراة ودية هامة استعدادا للتصفيات القادمة.",
    "افتتح معرض الفن التشكيلي أبوابه للجمهور اليوم في قاعة المدينة الكبرى.",
    "شهدت منطقة الأطلس الكبير تساقطات ثلجية كثيفة خلال عطلة نهاية الأسبوع.",
    "أعلنت وزارة الصحة عن تسجيل حالات إصابة جديدة بفيروس كورونا في عدة مدن.",
    "ناقش البرلمان اليوم مشروع قانون جديد يتعلق بتنظيم الإعلام والصحافة.",
    "انطلقت فعاليات المهرجان الدولي للفيلم في دورته السنوية بمشاركة فنانين عالميين.",
    "توقعات الأرصاد الجوية تشير إلى استمرار الأجواء الباردة في معظم مناطق المملكة.",
    "نظمت جمعية المجتمع المدني حملة نظافة واسعة في الأحياء السكنية.",
    "أسعار النفط تعرف ارتفاعا ملحوظا في الأسواق العالمية خلال الأيام الماضية.",
    "وقع حادث سير مروع على الطريق السيار الرابط بين مدينتين كبيرتين.",
    "احتفلت الجالية المغربية في الخارج بالذكرى السادسة والأربعين للمسيرة الخضراء.",
    "قناة تلفزيونية جديدة تنطلق قريبا مع باقة متنوعة من البرامج الإخبارية والثقافية.",
    "ندوة علمية تناقش آخر التطورات في مجال الطاقة المتجددة في جامعة محمد الخامس.",
    "فرق الإنقاذ تواصل البحث عن شخص مفقود في جبال الريف.",
    "أقيمت مباراة خيرية في كرة السلة لدعم الأسر المعوزة في المدينة.",
    "تنظم وزارة الثقافة ورشات فنية للأطفال خلال العطلة الربيعية.",
    "عرفت أسواق الجملة ارتفاعا في أسعار بعض المواد الأساسية.",
    "تم إلقاء القبض على عصابة إجرامية متخصصة في سرقة السيارات الفارهة.",
    "أصدرت وزارة الداخلية بلاغا بخصوص الإجراءات الجديدة المتعلقة بالحالة الصحية.",
    "تغطية إعلامية خاصة لزيارة الوفد الأجنبي الرفيع المستوى إلى المغرب.",
    "مؤتمر دولي حول التغيرات المناخية ينعقد في مراكش الأسبوع القادم.",
    "حملة تبرع بالدم تعرف إقبالا كبيرا من طرف المواطنين في مختلف المدن.",
    "البورصة المغربية تسجل ارتفاعا في قيمة الأسهم للشركات الكبرى.",
    "نجحت عناصر الشرطة في تفكيك شبكة للاتجار بالمخدرات الصلبة.",
    "احتفالية كبرى بمناسبة رأس السنة الأمازيغية في الأقاليم الجنوبية.",
    "مسابقة ثقافية وفنية تنظمها جمعية فاعلة في مجال الفنون التشكيلية.",
    "تقرير جديد يرصد التحديات التي تواجه قطاع السياحة في المغرب.",
    "تفاصيل جديدة حول جريمة قتل بشعة هزت الرأي العام المحلي.",
    "اتحاد الكرة يعلن عن قائمة اللاعبين المدعوين للمنتخب المحلي.",
    "عرض مسرحي جديد يقدم على خشبة المسرح الوطني محمد الخامس.",
    "توقعات بوصول أمطار الخير إلى المناطق الفلاحية خلال الأيام القادمة.",
    "ورشة عمل حول التنمية المستدامة تنظم بمشاركة خبراء دوليين.",
    "أسعار الخضر والفواكه تعرف استقرارا نسبيا في الأسواق المحلية.",
    "إحباط محاولة تهريب كمية كبيرة من السجائر المهربة.",
    "ندوة صحفية لوزير التربية الوطنية حول مستجدات الدخول المدرسي.",
    "برنامج تلفزيوني جديد يسلط الضوء على قصص نجاح شباب مغربي.",
    "معرض للكتب ينظم في المكتبة الوطنية بالرباط.",
    "عملية أمنية مكثفة تسفر عن اعتقال عدد من المبحوث عنهم.",
    "عقد اجتماع هام بين ممثلي الحكومة والمركزيات النقابية.",
],
        'topic': [
            "Economy", "Sport", "Art and Culture", "Regions", "Society",
            "Politics", "Art and Culture", "Regions", "Society", "Economy",
            "Miscellaneous/Current Events", "Moroccans of the World", "Media", "Orbits", "Regions",
            "Sport", "Art and Culture", "Economy", "Miscellaneous/Current Events", "Politics",
            "Media", "Orbits", "Society", "Economy", "Miscellaneous/Current Events",
            "Tamazight", "Art and Culture", "Economy", "Miscellaneous/Current Events", "Sport",
            "Art and Culture", "Regions", "Society", "Economy", "Miscellaneous/Current Events",
            "Politics", "Media", "Art and Culture", "Miscellaneous/Current Events", "Politics"
        ]}
custom_df = pd.DataFrame(data)
print("Larger dummy custom data created.")
print("Columns in the custom DataFrame:", custom_df.columns.tolist())
display(custom_df.head())

"""## Preprocess custom data

### Subtask:
Apply the same cleaning function (`clean`) and topic mapping (`topic_ids`) to your custom data as was done for the training data.

**Reasoning**:
Apply the cleaning function and topic mapping to the custom data.
"""

custom_df['story'] = custom_df['story'].apply(lambda x: clean(x))
custom_df['topic_id'] = custom_df['topic'].replace(topic_ids).astype(int)
display(custom_df.head())

"""## Tokenize custom data

### Subtask:
Use the same tokenizer (`tokenizer`) to tokenize your custom data.

**Reasoning**:
Tokenize the 'story' column of the custom_df DataFrame using the previously defined tokenizer.
"""

custom_encodings = tokenizer(list(custom_df['story'].values), truncation=True, padding='max_length', max_length=max_lenght)

"""**Reasoning**:
Evaluate the loaded model on the custom data using the compute_accuracy function and print the result.
"""

custom_accuracy = compute_accuracy(loaded_model, custom_loader, device)
print(f"Custom data accuracy: {custom_accuracy:.2f}%")